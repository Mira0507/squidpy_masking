---
title: "Building ImageContainer object"
author: "Mira Sohn"
output:
    html_document:
        code_folding: hide
        df_print: paged
        # toc: true
        # toc_float: true
        # toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      cache.lazy=FALSE)
```

Last run: `r date()`

This workflow is designed to demonstrate image segmentation for Visium analysis 
using the [squidpy (Palla, Spitzer et al., 2022)](https://doi.org/10.1038/s41592-021-01358-2) package.
Refer to the following resources for technical details:

- [Squidpy documentation](https://squidpy.readthedocs.io/en/stable/index.html)
- [Tutorial: Analyze Visium fluorescence data](https://squidpy.readthedocs.io/en/stable/notebooks/tutorials/tutorial_visium_fluo.html)
- [Squidpy GitHub](https://github.com/scverse/squidpy)

```{r rpackages}
library(reticulate)
library(tidyverse)
use_condaenv(params[['conda_env']])
```

```{python ppackages}
import numpy as np
import ast
import matplotlib.pyplot as plt
import squidpy as sq
from matplotlib.colors import ListedColormap
from skimage.filters import threshold_local
from skimage.morphology import binary_erosion
from skimage.exposure import equalize_adapthist
import xarray as xr
import os
```

```{python configs}

# ------------------------------------------------------------------------------
# This chunk is used to specify variables for paths to input files/directories
# and parameter settings
# ------------------------------------------------------------------------------

# reticulate::repl_python()

# Convert `params` from R to Python
params = r.params

# Specify file paths to input and output files
outdir = params['outdir']
input_image = params['input_image']
output_obj = params['output_obj']

# Specify the name of input image layer to be processed
lyr = params['lyr']

# Specify whether to equalize contrasts
equalize = params['equalize']

# Specify whether input images will be cropped
crop_images = params['crop_images']

# Specify coordinates to crop images
crop_height = params['crop_height']
crop_width = params['crop_width']
crop_size = params['crop_size']
crop_scale = params['crop_scale']
```

# Loading input images {.tabset}

We use `tif` image files converted from `vsi`. The following channels correspond to individual IFs.

- **Channel 0**: DAPI
- **Channel 1**: IBA1
- **Channel 2**: TDP43
- **Channel 3**: MAP2
- **Channel 4**: ALDH1L1

The following image object loaded:

```{python load_image}

# ------------------------------------------------------------------------------
# This chunk is used to load input image files
# ------------------------------------------------------------------------------

# Initialize ImageContainer
img = sq.im.ImageContainer(input_image, layer=lyr)

# Crop input images if specified
if crop_images == "Y":
    print("Image cropped. Refer to the following dimension:", crop_coord)
    img = img.crop_corner(x=crop_width,
                          y=crop_height,
                          size=crop_size,
                          scale=crop_scale)
    print("ImageContainer obj updated:", img)

# Equalize of specified
if equalize == "Y":
    # Run adaptive equalization
    eq_img = equalize_adapthist(img[lyr])
    # Convert to xarray
    # NOTE: This step requires large memory and long runtime!
    eq_img = xr.DataArray(eq_img)
    # Add the input array with equalized array
    eq_lyr = f"eq_{lyr}"
    img[lyr] = eq_img
    print("Adaptive equalization applied to the input image!")
```

`ImageContainer` obj created using input image:

```{python print_status}
print(input_image)
print(img)
```

Contrast enhancement is conducted using the `skimage.exposure.equalize_adapthist`
function with default parameter settings.

```{r vis_input, results='asis'}

# ------------------------------------------------------------------------------
# This chunk iteratively visualizes images before and after smoothing
# ------------------------------------------------------------------------------

# Function to generate a fake python chunk running python commands
subchunkify <- function(name, layer, channel, t_deparsed, width=12, height=12) {

    more <- paste0(", fig.width=", width, ", fig.height=", height)
    sub_chunk <- paste0("```{python ", name, ", results='asis', echo=FALSE", more, "}",
        "\n\n",
        paste0(t_deparsed, collapse="\n"),
        "\n\n```\n\n\n")

    cat(knitr::knit(text = sub_chunk, quiet=TRUE))
}

# Function to add a link to image
link_output <- function(p) {
    cat("\n\n- Link: [", p, "](", p, ")\n\n")
}

# Visualize input images across the channels
for (i in 1:py$img[[py$lyr]][['shape']][[4]]) {
    cat("## Channel", i, "{.tabset}\n\n")
    # Command to show image
    t_deparsed_1 <- paste0("img.show('", py$lyr, "', channel=", i, ", cmap='gray')")
    # Command to save image
    t_deparsed_2 <- paste0("plt.savefig('", py$outdir, "/input_channel_", i,  ".png', dpi=1000)")
    t_deparsed <- c(t_deparsed_1, t_deparsed_2)
    subchunkify(name=paste0("input_", i),
                t_deparsed=t_deparsed,
                layer=py$lyr,
                channel=i)

    save_path <- paste0(py$outdir, "/input_channel_", i, ".png")
    link_output(save_path)
}
```


# Saving the `ImageContainer` object

Output images are saved as the following object:

```{python save_zarr}
img.save(output_obj)
print(output_obj)
```

```{r session_info, collapse=FALSE}
sessionInfo()
```
