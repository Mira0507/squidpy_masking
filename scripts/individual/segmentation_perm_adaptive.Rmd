---
title: "Segmentation using squidpy"
author: "Mira Sohn"
output:
    html_document:
        code_folding: hide
        df_print: paged
        # toc: true
        # toc_float: true
        # toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE,
                      message=FALSE,
                      cache.lazy=FALSE)
```

Last run: `r date()`

This workflow is designed to demonstrate image segmentation for Visium analysis 
using the [squidpy (Palla, Spitzer et al., 2022)](https://doi.org/10.1038/s41592-021-01358-2) package.
Refer to the following resources for technical details:

- [Squidpy documentation](https://squidpy.readthedocs.io/en/stable/index.html)
- [Tutorial: Analyze Visium fluorescence data](https://squidpy.readthedocs.io/en/stable/notebooks/tutorials/tutorial_visium_fluo.html)
- [Squidpy GitHub](https://github.com/scverse/squidpy)

```{r rpackages}
library(reticulate)
library(tidyverse)
use_condaenv('../../env')
```

```{python ppackages}
import numpy as np
import matplotlib.pyplot as plt
import squidpy as sq
from matplotlib.colors import ListedColormap
from skimage.filters import threshold_local
from skimage.morphology import binary_erosion
from skimage.exposure import equalize_adapthist
import xarray as xr
import napari
import os
```

```{python configs}

# ------------------------------------------------------------------------------
# This chunk is used to specify variables for paths to input files/directories
# and parameter settings
# ------------------------------------------------------------------------------

# reticulate::repl_python()

# Specify file paths to input and output files
outdir = "segmentation_perm_adaptive"
input_image = "../../images/converted/perm/converted.ome.tif"
output_obj = f"{outdir}/icontainer.zarr"

# Create the output directory 
os.makedirs(outdir, exist_ok=True)

# Specify the name of input image layer to be processed
lyr = 'image'

# Specify signal values to be tested for Gaussian smoothing
# Sigma represents the variance for normal distribution
gaussian_sigma = 1

# Specify whether to equalize contrasts
equalize = False

# Specify whether input images will be cropped
crop_images = False

# Specify coordinates to crop images
crop_coord = {'height' : 0.5, 'width' : 0.5, 'size' : 1000, 'scale' : 1}

# Specify the segmentation method
seg_method = "watershed"

# Specify the thresholding method. If `None`, the Otsu method is used.
thr_method = None

# Set parameters for adaptive thresholding
# - block_size: neighborhood size, ~5â€“15% of the image dimension, odd integer.
# - offset: a constant added to (or subtracted from) the computed threshold (set to 0 by default)
#           higher offset results in more stringent thresholding
block_size = round(crop_coord['size'] / 2) + 1
offset = -100

# Specify pseudocolors across the channels
col_dic = {
    'dapi': 'dodgerblue',
    'iba': 'rosybrown',
    'tdp': 'orange',
    'map': 'cyan',
    'aldh': 'darkkhaki'}

# Specify merges
merge_list = ['dapi_tdp', 'iba_map_aldh', 'tdp_map']

# Specify the prefix of layers being merged
merge_prefix = "erosion"
```

# Loading input images {.tabset}

We use `tif` image files converted from `vsi`. Refer to the [image_conversion.html](image_conversion.html) 
for detailed processes on converting image formats. The following channels correspond to individual IFs.

- **Channel 0**: DAPI
- **Channel 1**: IBA1
- **Channel 2**: TDP43
- **Channel 3**: MAP2
- **Channel 4**: ALDH1L1

The following image object loaded:

```{python load_image}

# ------------------------------------------------------------------------------
# This chunk is used to load input image files
# ------------------------------------------------------------------------------

# Initialize ImageContainer
img = sq.im.ImageContainer(input_image, layer=lyr)

print("ImageContainer obj created using input image:", img)

# Crop input images if specified
if crop_images:
    print("Image cropped. Refer to the following dimension:", crop_coord)
    img = img.crop_corner(x=crop_coord['width'],
                          y=crop_coord['height'],
                          size=crop_coord['size'],
                          scale=crop_coord['scale'])
    print("ImageContainer obj updated:", img)

# Equalize of specified
if equalize:
    # Run adaptive equalization
    eq_img = equalize_adapthist(img[lyr])
    # Convert to xarray
    eq_img = xr.DataArray(eq_img)
    # Replace the input array with equalized array
    img[lyr] = eq_img
    print("Adaptive equalization applied to the input image!")

```

```{r vis_input, results='asis'}

# ------------------------------------------------------------------------------
# This chunk iteratively visualizes images before and after smoothing
# ------------------------------------------------------------------------------

# Function to generate a fake python chunk running python commands
subchunkify <- function(name, layer, channel, t_deparsed, width=12, height=12) {

    more <- paste0(", fig.width=", width, ", fig.height=", height)
    sub_chunk <- paste0("```{python ", name, ", results='asis', echo=FALSE", more, "}",
        "\n\n",
        paste0(t_deparsed, collapse="\n"),
        "\n\n```\n\n\n")

    cat(knitr::knit(text = sub_chunk, quiet=TRUE))
}

# Visualize input images across the channels
for (i in 1:py$img[[py$lyr]][['shape']][[4]]) {
    cat("## Channel", i, "{.tabset}\n\n")
    # Command to show image
    t_deparsed_1 <- paste0("img.show('", py$lyr, "', channel=", i, ", cmap='gray')")
    # Command to save image
    t_deparsed_2 <- paste0("plt.savefig('", py$outdir, "/input_channel_", i,  ".png', dpi=1000)")
    t_deparsed <- c(t_deparsed_1, t_deparsed_2)
    subchunkify(name=paste0("input_", i),
                t_deparsed=t_deparsed,
                layer=py$lyr,
                channel=i)
}
```

```{r session_info, collapse=FALSE}
sessionInfo()
```
